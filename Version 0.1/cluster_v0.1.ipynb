{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering v0.1\n",
    "This notebook outlines how the features derived in Feature Extraction v0.1 are used to find clusters of passenger travel patterns across bus services. The key idea of the clustering is that the clusters will capture services where passengers have similar travel patterns and one could apply a \"\"global\" intervention on the entire cluster to improve bus services. \n",
    "\n",
    "As mentioned in Feature Extraction v0.1, the features are of uneven length. Some are as short as 12 dimensions while others can be as long as 65. As such, traditional methods of distance measures between vectors fail as they assume feature vectors of even length. However, a feature vector of 60 dimensions can have a similar feature waveform with one that is of 20 dimensions. Ideally, they should fall in the same cluster.\n",
    "\n",
    "Here, I use **Dynamic Time Warping (DTW)** as the form of distance measure.  DTW is a time-series analysis concept which measures similarity between two temporal sequences, which may vary in speed. For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data.In short, any data that can be turned into a linear sequence can be analyzed with DTW. \n",
    "\n",
    "Since the feature vectors created are linear sequences of bus stops, DTW is quite apt for measuring similarity between feature vectors of different lengths. Using DTW, the distance matrix can be generated which in turn can be passed to traditional clustering approaches like K-Means, DBScan etc to find clusters. In this notebook, I use *Hierarchical Clustering* to find clusters. I find the number of clusters by determining where the biggest jump in distance occurs between aggregation of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./service_profile.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commands above load the external functions, classes and libraries needed by this notebook to run. The functions below connect to the Amazon S3 storage which contains the data and establishes a verified connection. After connecting, it mounts the volume onto the Databricks file system for access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 bucket to use\n",
    "bucket = S3Bucket(\"bus-v2-data\", AWS_ACCESS_KEY, AWS_SECRET_KEY)\n",
    "bucket.allowSpark().mount('s3', ignore_exception=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the data schemas for data source that contains the feature vectors and load the data with the schema defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resources\n",
    "path = bucket.s3(\"workspace/amit/bus_dist_series_data_0.1.csv\")\n",
    "schema = StructType([\n",
    "    StructField('Feature', DoubleType()),\n",
    "    StructField('bus stop code', StringType()),\n",
    "    StructField('direction', IntegerType()),\n",
    "    StructField('service', StringType()),\n",
    "])\n",
    "df = spark.read.csv(path, header=\"true\", schema=schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the list of bus services and directions from the data. Convert the Spark dataframe to a Pandas dataframe for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain trunk bus services in data and corresponding directions\n",
    "bus_srvc_direc = df.select('service','direction').distinct().rdd.map(lambda r: (r[0], r[1])).collect()\n",
    "\n",
    "# Convert spark dataframe to pandas\n",
    "dist_series_data = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance matrix computation\n",
    "The distance matrix is initialized below with the columns and rows representing the various bus services and direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DTW distances between all features and compute a 2D distance matrix for clustering\n",
    "# Create empty pandas dataframe to store all the distances with the indices labelled by the bus service and direction\n",
    "mat_sz = len(bus_srvc_direc)\n",
    "I = pd.Index(bus_srvc_direc, name=\"\")\n",
    "C = pd.Index(bus_srvc_direc, name=\"\")\n",
    "dtw_distance_mat = pd.DataFrame(pd.np.zeros((mat_sz, mat_sz), dtype=np.float), index=I, columns=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are extracted for each bus service and direction from the Pandas dataframe into a list of numpy arrays. This is for easy indexing for distance computation later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature list for easy indexing\n",
    "feature_list = []\n",
    "for item in bus_srvc_direc:\n",
    "    feature = dist_series_data.loc[(dist_series_data['service'] == item[0]) & (dist_series_data['direction'] == item[1]), 'Feature'].values\n",
    "    feature = feature.reshape((feature.shape[0], ))\n",
    "    feature_list.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance matrix is computed using a nested for loop over all the feature vectors. The distance returned is added to the matrix initialized earlier. To note, due to diagonal symmetry of the distance matrix, the code below only computes the upper half and replicates the results for the lower half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distance matrix for feature list\n",
    "for i in range(0, len(feature_list)):\n",
    "    for j in range(i + 1, len(feature_list)):\n",
    "        distance, path = fastdtw(feature_list[i], feature_list[j], dist=euclidean)\n",
    "        dtw_distance_mat.loc[(bus_srvc_direc[i][0], bus_srvc_direc[i][1]),(bus_srvc_direc[j][0],bus_srvc_direc[j][1])] = distance\n",
    "        dtw_distance_mat.loc[(bus_srvc_direc[j][0], bus_srvc_direc[j][1]),(bus_srvc_direc[i][0],bus_srvc_direc[i][1])] = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are then saved to the S3 storage for clustering analysis without the need to repeatedly find the distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "df = spark.createDataFrame(dtw_distance_mat)\n",
    "\n",
    "# Save Spark DataFrame to S3\n",
    "df.coalesce(1).write.format('com.databricks.spark.csv').options(header='true').save('s3a://bus-v2-data/workspace/amit/dtw_distance_mat_0.1.csv',mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the clusters of passenger travel patterns across all bus services\n",
    "The DTW distance matrix computed earlier is loaded from the S3 storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 bucket to use\n",
    "bucket = S3Bucket(\"bus-v2-data\", AWS_ACCESS_KEY, AWS_SECRET_KEY)\n",
    "bucket.allowSpark().mount('s3', ignore_exception=True)\n",
    "\n",
    "# util func\n",
    "ws = lambda path: \"/workspace/amit/\" + path  # return path to my workspace\n",
    "\n",
    "path = bucket.s3(\"workspace/amit/dtw_distance_mat_0.1.csv\")\n",
    "df = sqlContext.read.format('csv').options(header='true', inferSchema='true').load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the libraries needed for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the EZ-Link data and the route information data and initialize their data schemas. Also, the variables needed for filtering the data is initialized here. The reason we are doing so is that after clustering, we will need all this information to get the trips information for generation of the vega diagrams for the services belonging to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resources\n",
    "EZLINK = bucket.s3(\"data/ezlink-201702.parquet\")\n",
    "ROUTE = bucket.local(\"data/lta_scheduled_bus_routes_for_feb2017.csv\")\n",
    "\n",
    "from datetime import datetime, time\n",
    "\n",
    "route_schema = dict(\n",
    "    service=\"service\",\n",
    "    direction=\"direction\",\n",
    "    stop_code=\"BusStopCode\",\n",
    "    seq=\"BusStopSequence\",\n",
    "    km=\"km\",\n",
    "    dt_from=\"dt_from\",\n",
    "    dt_to=\"dt_to\",\n",
    "    time_format='%d/%m/%Y')\n",
    "\n",
    "ezlink_schema = dict(\n",
    "    src=\"BOARDING_STOP_STN\",\n",
    "    dst=\"ALIGHTING_STOP_STN\",\n",
    "    year=\"Year\",\n",
    "    bus_id=\"BUS_REG_NUM\",\n",
    "    trip_id=\"Bus_Trip_Num\",\n",
    "    journey_id=\"JOURNEY_ID\",\n",
    "    travel_mode=\"TRAVEL_MODE\",\n",
    "    service=\"Srvc_Number\",\n",
    "    direction=\"Direction\",\n",
    "    km=\"Ride_Distance\",\n",
    "    tap_in_time=\"tap_in_time\",\n",
    "    tap_out_time=\"tap_out_time\")\n",
    "\n",
    "route_valid_for_date = datetime(2017, 2, 14)\n",
    "days_of_interest = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    "am_peak = dict(start_time=time(7, 30), end_time=time(9, 30))\n",
    "\n",
    "# route data\n",
    "route = (Route.from_csv(ROUTE, **route_schema).valid_for(route_valid_for_date))\n",
    "\n",
    "# ezlink data\n",
    "ezlink_data = spark.read.parquet(EZLINK)\n",
    "\n",
    "# Subset bus data\n",
    "ezlink_data.createOrReplaceTempView('data_table')\n",
    "ezlink_bus_data = sqlContext.sql('select * from data_table where TRAVEL_MODE=\"Bus\"')\n",
    "ezlink = Ezlink(ezlink_bus_data, **ezlink_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert spark dataframe to pandas\n",
    "dtw_distance_mat = df.toPandas()\n",
    "ezlink_bus_srvc = tuple(dtw_distance_mat.columns.values)\n",
    "idx = dict(enumerate(ezlink_bus_srvc, start=0))\n",
    "dtw_distance_mat.rename(index=idx, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the distance matrix to a form required by the sci-kit learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_distance_mat_condensed = squareform(dtw_distance_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform hierarchical clustering on the distance matrix and generate the dendogram to identify what the suitable number of clusters should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the linkage matrix\n",
    "Z = linkage(dtw_distance_mat_condensed, 'ward')\n",
    "plt.figure(figsize=(100, 40))\n",
    "plt.title('Hierarchical Clustering Ward Linkage Dendrogram of Commuter-Bus Travel Patterns',fontsize=36)\n",
    "plt.xlabel('sample index', fontsize=36)\n",
    "plt.ylabel('distance', fontsize=36)\n",
    "dn = dendrogram(Z, labels=dtw_distance_mat.index, leaf_font_size=16.)\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cluster number determined, generate the feature plots and vega diagram plots of each bus service and direction within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "fig = plt.figure(figsize=(100, 40))\n",
    "for clust_num in range(5, 17):\n",
    "    nodes = fcluster(Z, clust_num, criterion=\"maxclust\")\n",
    "    df = pd.DataFrame({\n",
    "        \"bus service direc\": bus_srvc_direc,\n",
    "        \"feature\": feature_list,\n",
    "        \"cluster\": nodes\n",
    "    })\n",
    "    for cluster in range(1, clust_num + 1):\n",
    "        df_sub = df.loc[df[\"cluster\"] == cluster]\n",
    "        dbutils.fs.mkdirs(bucket.s3(\"workspace/amit/plots_0.1/Clusters=\" + str(clust_num) + \"/\" + str(cluster)))\n",
    "        SAVED_PLOTS_LOCATION = bucket.local(\"workspace/amit/plots_0.1/Clusters=\" + str(clust_num) + \"/\" + str(cluster))\n",
    "        for ind, feat in enumerate(df_sub[\"feature\"]):\n",
    "            # Save feature plot of each service and direction for the cluster\n",
    "            bus_stops = dist_series_data.loc[(dist_series_data['service'] == df_sub.iloc[ind, 0][0]) & (dist_series_data['direction'] == df_sub.iloc[ind, 0][1]), 'bus stop code'].values\n",
    "            y_pos = np.arange(len(bus_stops))\n",
    "            plt.bar(y_pos, feat, align='center', alpha=0.5)\n",
    "            plt.xticks(y_pos, bus_stops, rotation='vertical')\n",
    "            plt.title('Passenger Flow vs Stops for Service {} Direction {} in Cluster: {}'.format(df_sub.iloc[ind, 0][0], df_sub.iloc[ind, 0][1],cluster),fontsize=96)\n",
    "            plt.xlabel('Bus Stops', fontsize=96)\n",
    "            plt.ylabel('Normalized Passenger Flow', fontsize=96)\n",
    "            ax = plt.gca()\n",
    "            ax.tick_params(axis='both', which='major', labelsize=64)\n",
    "            fig.savefig(SAVED_PLOTS_LOCATION + '/' + str(df_sub.iloc[ind, 0][0]) + '_' + str(df_sub.iloc[ind, 0][1]) + '.png', bbox_inches='tight')\n",
    "            plt.clf()\n",
    "\n",
    "            # Save corresponding vega diagram along with feature plot\n",
    "            srvc = dict(service=df_sub.iloc[ind, 0][0],direction=df_sub.iloc[ind, 0][1])\n",
    "            service_route = route.for_service(**srvc)\n",
    "            trips = (ezlink.for_service(**srvc).in_days_of_week(days_of_interest).within_time_range(**am_peak).get_trips(service_route))\n",
    "            edges = trips.to_edges(source=\"src_seq\", target=\"dst_seq\", value=\"pax\")\n",
    "            nodes = service_route.to_nodes(name=\"stop_code\", index=\"seq\", order=\"km\")\n",
    "            arc_html = vega(arc_diagram(edges=edges, nodes=nodes), render=False)\n",
    "            arc_file = open(SAVED_PLOTS_LOCATION + '/' + str(df_sub.iloc[ind, 0][0]) + '_' + str(df_sub.iloc[ind, 0][1]) + '_vega_arc.html', \"w\")\n",
    "            arc_file.write(arc_html)\n",
    "            arc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues with the DTW approach for clustering\n",
    "The problem with this approach was that the clusters generated were not understandable. On deeper analysis, I found that DTW basically normalizes phase shifts between feature vectors. So a feature vector with 1 triangular peak at around 25% of the journey will become highly similar with one that has 1 triangular peak at around 75% or 50% or even 90% because DTW compresses/expands the waveform during the distance computation to match. But, clearly, the feature vectors mentioned above are very different from one another.\n",
    "\n",
    "There are no other distance measure approaches in literature at the moment that handle distance computation between features of uneven length."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
