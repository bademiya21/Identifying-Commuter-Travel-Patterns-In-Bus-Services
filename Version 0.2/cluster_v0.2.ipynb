{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering v0.2\n",
    "This notebook outlines how the features derived in Feature Extraction v0.2 are used to find clusters of passenger travel patterns across bus services. The key idea of the clustering is that the clusters will capture services where passengers have similar travel patterns and one could apply a \"\"global\" intervention on the entire cluster to improve bus services. \n",
    "\n",
    "The features are of equal length for all bus services and direction. As such, we can directly apply traditional clustering approaches to the features using sci-kit learn package. The distance measure is set as euclidean. Here, KMeans clustering is used as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./service_profile.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commands above load the external functions, classes and libraries needed by this notebook to run. The functions below connect to the Amazon S3 storage which contains the data and establishes a verified connection. After connecting, it mounts the volume onto the Databricks file system for access. The data schemas for the data sources are also defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 bucket to use\n",
    "bucket = S3Bucket(\"bus-v2-data\", AWS_ACCESS_KEY, AWS_SECRET_KEY)\n",
    "bucket.allowSpark().mount('s3', ignore_exception=True)\n",
    "\n",
    "# resources\n",
    "path = bucket.s3(\"workspace/amit/bus_dist_series_data_0.2.csv\")\n",
    "schema = StructType([\n",
    "    StructField('Feature', DoubleType()),\n",
    "    StructField('direction', IntegerType()),\n",
    "    StructField('service', StringType()),\n",
    "])\n",
    "df = spark.read.csv(path, header=\"true\", schema=schema)\n",
    "display(df)\n",
    "\n",
    "# resources\n",
    "path = bucket.s3(\"workspace/amit/bus_dist_series_data_loop_0.2.csv\")\n",
    "schema = StructType([\n",
    "    StructField('Feature', DoubleType()),\n",
    "    StructField('direction', IntegerType()),\n",
    "    StructField('service', StringType()),\n",
    "])\n",
    "df_loop = spark.read.csv(path, header=\"true\", schema=schema)\n",
    "display(df_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the list of bus services and directions from the data. Convert the Spark dataframes to Pandas dataframes for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain trunk bus services in data and corresponding directions\n",
    "bus_srvc_direc = df.select('service','direction').distinct().rdd.map(lambda r: (r[0], r[1])).collect()\n",
    "bus_srvc_direc_loop = df_loop.select('service','direction').distinct().rdd.map(lambda r: (r[0], r[1])).collect()\n",
    "\n",
    "# Convert spark dataframe to pandas\n",
    "dist_series_data = df.toPandas()\n",
    "dist_series_data_loop = df_loop.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering using KMeans\n",
    "The feature list is extracted and transformer to list of numpy arrays for easy indexing for later operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature list for easy indexing\n",
    "feature_list = []\n",
    "for item in bus_srvc_direc:\n",
    "    feature = dist_series_data.loc[(dist_series_data['service'] == item[0]) & (dist_series_data['direction'] == item[1]), 'Feature'].values\n",
    "    feature = feature.reshape(1, -1)\n",
    "    feature_list.append(feature)\n",
    "feature_list = np.concatenate(feature_list, axis=0)\n",
    "\n",
    "# Create feature list for easy indexing\n",
    "feature_list_loop = []\n",
    "for item in bus_srvc_direc_loop:\n",
    "    feature = dist_series_data_loop.loc[(dist_series_data_loop['service'] == item[0]) & (dist_series_data_loop['direction'] == item[1]), 'Feature'].values\n",
    "    feature = feature.reshape(1, -1)\n",
    "    feature_list_loop.append(feature)\n",
    "feature_list_loop = np.concatenate(feature_list_loop, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries needed for clustering and visualization are loaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the EZ-Link data and the route information data and initialize their data schemas. Also, the variables needed for filtering the data is initialized here. The reason we are doing so is that after clustering, we will need all this information to get the trips information for generation of the vega diagrams for the services belonging to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resources\n",
    "EZLINK = bucket.s3(\"data/ezlink-201702.parquet\")\n",
    "ROUTE = bucket.local(\"data/lta_scheduled_bus_routes_for_feb2017.csv\")\n",
    "\n",
    "from datetime import datetime, time\n",
    "\n",
    "route_schema = dict(\n",
    "    service=\"service\",\n",
    "    direction=\"direction\",\n",
    "    stop_code=\"BusStopCode\",\n",
    "    seq=\"BusStopSequence\",\n",
    "    km=\"km\",\n",
    "    dt_from=\"dt_from\",\n",
    "    dt_to=\"dt_to\",\n",
    "    time_format='%d/%m/%Y')\n",
    "\n",
    "ezlink_schema = dict(\n",
    "    src=\"BOARDING_STOP_STN\",\n",
    "    dst=\"ALIGHTING_STOP_STN\",\n",
    "    year=\"Year\",\n",
    "    bus_id=\"BUS_REG_NUM\",\n",
    "    trip_id=\"Bus_Trip_Num\",\n",
    "    journey_id=\"JOURNEY_ID\",\n",
    "    travel_mode=\"TRAVEL_MODE\",\n",
    "    service=\"Srvc_Number\",\n",
    "    direction=\"Direction\",\n",
    "    km=\"Ride_Distance\",\n",
    "    tap_in_time=\"tap_in_time\",\n",
    "    tap_out_time=\"tap_out_time\")\n",
    "\n",
    "route_valid_for_date = datetime(2017, 2, 14)\n",
    "days_of_interest = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    "am_peak = dict(start_time=time(7, 30), end_time=time(9, 30))\n",
    "\n",
    "# route data\n",
    "route = (Route.from_csv(ROUTE, **route_schema).valid_for(route_valid_for_date))\n",
    "\n",
    "# ezlink data\n",
    "ezlink_data = spark.read.parquet(EZLINK)\n",
    "\n",
    "# Subset bus data\n",
    "ezlink_data.createOrReplaceTempView('data_table')\n",
    "ezlink_bus_data = sqlContext.sql('select * from data_table where TRAVEL_MODE=\"Bus\"')\n",
    "ezlink = Ezlink(ezlink_bus_data, **ezlink_schema)\n",
    "ezlink = (ezlink.in_days_of_week(days_of_interest).within_time_range(**am_peak))\n",
    "ezlink.dataframe.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the appropriate number of clusters\n",
    "As KMeans requires a user input for the number of clusters to find, it is appropriate to select a range of values to perform clustering over and choose the best one based on some metric measure. Here, I show using silhouette scores.\n",
    "\n",
    "Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The silhouette value is a measure of how similar an object is to its own cluster compared to other clusters. The silhouette ranges from âˆ’1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance.\n",
    "\n",
    "For each cluster number, I find the average silhouette score of all the clusters. The one with the highest value is, by right, supposed to be the best number of clusters for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clust_num in range(2, 41):\n",
    "    labels = KMeans(\n",
    "        n_clusters=clust_num,\n",
    "        init='random',\n",
    "        max_iter=150000,\n",
    "        n_init=1000,\n",
    "        random_state=1,\n",
    "        n_jobs=-1).fit_predict(feature_list)\n",
    "    labels_loop = KMeans(\n",
    "        n_clusters=clust_num,\n",
    "        init='random',\n",
    "        max_iter=150000,\n",
    "        n_init=1000,\n",
    "        random_state=1,\n",
    "        n_jobs=-1).fit_predict(feature_list_loop)\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(feature_list, labels)\n",
    "    print(\"For n_clusters =\", clust_num,\"The average silhouette_score for non-loop services is :\",silhouette_avg)\n",
    "    silhouette_avg = silhouette_score(feature_list_loop, labels_loop)\n",
    "    print(\"For n_clusters =\", clust_num,\"The average silhouette_score for loop services is :\",silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, instead of relying of quantitative metrics to determine cluster numbers, we could use qualitative metrics i.e. visualization to see if the number of clusters chosen does actually separate the data nicely. Below, I use Principal Component Analysis (PCA) to reduce the data into 2-3 dimensions for visualization. This may not be the best way to visualize the clustered data but it can give an inclination as to whether the cluster numbers chosen make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_num = 17\n",
    "labels = KMeans(\n",
    "    n_clusters=clust_num,\n",
    "    init='random',\n",
    "    max_iter=150000,\n",
    "    n_init=1000,\n",
    "    random_state=1,\n",
    "    n_jobs=-1).fit_predict(feature_list)\n",
    "clust_num_loop = 11\n",
    "labels_loop = KMeans(\n",
    "    n_clusters=clust_num_loop,\n",
    "    init='random',\n",
    "    max_iter=150000,\n",
    "    n_init=1000,\n",
    "    random_state=1,\n",
    "    n_jobs=-1).fit_predict(feature_list_loop)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import itertools\n",
    "\n",
    "pca1 = PCA(n_components=3)\n",
    "feature_list_r = pca1.fit(feature_list).transform(feature_list)\n",
    "pca2 = PCA(n_components=3)\n",
    "feature_list_loop_r = pca2.fit(feature_list_loop).transform(feature_list_loop)\n",
    "\n",
    "markers = itertools.cycle(('.', ',', 'o', 'v', '^', '<', '>', '1', '2', '3', '4', '8', 's', 'p', '*', 'h', 'H', '+', 'x', 'd', '|', '_'))\n",
    "colors_ = list(six.iteritems(colors.cnames))\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "\n",
    "ax = fig.add_subplot(221, projection='3d')\n",
    "\n",
    "for i in range(0, clust_num):\n",
    "    ax.scatter(\n",
    "        feature_list_r[labels == i, 0],\n",
    "        feature_list_r[labels == i, 1],\n",
    "        feature_list_r[labels == i, 2],\n",
    "        c=colors_[i][0],\n",
    "        marker=next(markers),\n",
    "        s=64)\n",
    "\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "ax.set_zlabel('PCA 3')\n",
    "\n",
    "ax.set_title('PCA of Features for Non-Loop Services')\n",
    "\n",
    "ax = fig.add_subplot(222, projection='3d')\n",
    "\n",
    "for i in range(0, clust_num_loop):\n",
    "    ax.scatter(\n",
    "        feature_list_loop_r[labels_loop == i, 0],\n",
    "        feature_list_loop_r[labels_loop == i, 1],\n",
    "        feature_list_loop_r[labels_loop == i, 2],\n",
    "        c=colors_[i][0],\n",
    "        marker=next(markers),\n",
    "        s=64)\n",
    "\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "ax.set_zlabel('PCA 3')\n",
    "\n",
    "ax.set_title('PCA of Features for Loop Services')\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "\n",
    "for i in range(0, clust_num):\n",
    "    plt.scatter(\n",
    "        feature_list_r[labels == i, 0],\n",
    "        feature_list_r[labels == i, 1],\n",
    "        c=colors_[i][0],\n",
    "        marker=next(markers),\n",
    "        s=64)\n",
    "\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "\n",
    "ax.set_title('PCA of Features for Non-Loop Services')\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "\n",
    "for i in range(0, clust_num_loop):\n",
    "    plt.scatter(\n",
    "        feature_list_loop_r[labels_loop == i, 0],\n",
    "        feature_list_loop_r[labels_loop == i, 1],\n",
    "        c=colors_[i][0],\n",
    "        marker=next(markers),\n",
    "        s=64)\n",
    "\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "\n",
    "ax.set_title('PCA of Features for Loop Services')\n",
    "\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cluster number determined, generate the feature plots and vega diagram plots of each bus service and direction within each cluster. The chunk below is for non-loop services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "for clust_num in range(25, 31):\n",
    "    labels = KMeans(\n",
    "        n_clusters=clust_num,\n",
    "        init='random',\n",
    "        max_iter=150000,\n",
    "        n_init=1000,\n",
    "        random_state=1,\n",
    "        n_jobs=-1).fit_predict(feature_list)\n",
    "    df = pd.DataFrame({\n",
    "        \"bus service direc\": bus_srvc_direc,\n",
    "        \"feature\": list(feature_list),\n",
    "        \"cluster\": labels.reshape(-1, )\n",
    "    })\n",
    "\n",
    "    for cluster in range(0, clust_num):\n",
    "        df_sub = df.loc[df[\"cluster\"] == cluster]\n",
    "        dbutils.fs.mkdirs(bucket.s3(\"workspace/amit/plots_0.2/KMeans/Non-Loop/Clusters=\" + str(clust_num) + \"/\" + str(cluster)))\n",
    "        SAVED_PLOTS_LOCATION = bucket.local(\"workspace/amit/plots_0.2/KMeans/Non-Loop/Clusters=\" + str(clust_num) + \"/\" + str(cluster))\n",
    "\n",
    "        for ind, feat in enumerate(df_sub[\"feature\"]):\n",
    "            fig = plt.figure(figsize=(100, 50))\n",
    "            # Save feature plot of each service and direction for the cluster\n",
    "            y_pos = np.arange(20)\n",
    "            plt.bar(y_pos, feat, alpha=0.5)\n",
    "            plt.title('Passenger Flow vs Normalized Distance for Service {} Direction {} in Cluster: {}'.format(df_sub.iloc[ind, 0][0], df_sub.iloc[ind, 0][1],cluster),fontsize=96)\n",
    "            plt.xlabel('Normalized Distance (1 km)', fontsize=96)\n",
    "            plt.ylabel('Normalized Passenger Flow', fontsize=96)\n",
    "            ax = plt.gca()\n",
    "            ax.tick_params(axis='both', which='major', labelsize=64)\n",
    "            fig.savefig(SAVED_PLOTS_LOCATION + '/' + str(df_sub.iloc[ind, 0][0]) + '_' + str(df_sub.iloc[ind, 0][1]) + '.png',bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            # Save corresponding vega diagram along with feature plot\n",
    "            srvc = dict(service=df_sub.iloc[ind, 0][0],direction=df_sub.iloc[ind, 0][1])\n",
    "            service_route = route.for_service(**srvc)\n",
    "            trips = (ezlink.for_service(**srvc).in_days_of_week(days_of_interest).within_time_range(**am_peak).get_trips(service_route))\n",
    "            edges = trips.to_edges(source=\"src_seq\", target=\"dst_seq\", value=\"pax\")\n",
    "            nodes = service_route.to_nodes(name=\"stop_code\", index=\"seq\", order=\"km\")\n",
    "            arc_html = vega(arc_diagram(edges=edges, nodes=nodes), render=False)\n",
    "            arc_file = open(SAVED_PLOTS_LOCATION + '/' + str(df_sub.iloc[ind, 0][0]) + '_' + str(df_sub.iloc[ind, 0][1]) + '_vega_arc.html', \"w\")\n",
    "            arc_file.write(arc_html)\n",
    "            arc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunk below is for loop services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clust_num_loop in range(6, 31):\n",
    "    labels_loop = KMeans(\n",
    "        n_clusters=clust_num_loop,\n",
    "        init='random',\n",
    "        max_iter=150000,\n",
    "        n_init=1000,\n",
    "        random_state=1,\n",
    "        n_jobs=-1).fit_predict(feature_list_loop)\n",
    "    df_loop = pd.DataFrame({\n",
    "        \"bus service direc\": bus_srvc_direc_loop,\n",
    "        \"feature\": list(feature_list_loop),\n",
    "        \"cluster\": labels_loop.reshape(-1, )\n",
    "    })\n",
    "    for cluster in range(0, clust_num_loop):\n",
    "        df_sub_loop = df_loop.loc[df_loop[\"cluster\"] == cluster]\n",
    "        dbutils.fs.mkdirs(bucket.s3(\"workspace/amit/plots_0.2/KMeans/Loop/Clusters=\" + str(clust_num_loop) + \"/\" + str(cluster)))\n",
    "        SAVED_PLOTS_LOCATION_LOOP = bucket.local(\"workspace/amit/plots_0.2/KMeans/Loop/Clusters=\" + str(clust_num_loop) + \"/\" + str(cluster))\n",
    "\n",
    "        for ind, feat in enumerate(df_sub_loop[\"feature\"]):\n",
    "            fig = plt.figure(figsize=(100, 50))\n",
    "            # Save feature plot of each service and direction for the cluster\n",
    "            y_pos = np.arange(20)\n",
    "            plt.bar(y_pos, feat, alpha=0.5)\n",
    "            plt.title('Passenger Flow vs Normalized Distance for Service {} Direction {} in Cluster: {}'.format(df_sub_loop.iloc[ind, 0][0],df_sub_loop.iloc[ind, 0][1], cluster),fontsize=96)\n",
    "            plt.xlabel('Normalized Distance (1 km)', fontsize=96)\n",
    "            plt.ylabel('Normalized Passenger Flow', fontsize=96)\n",
    "            ax = plt.gca()\n",
    "            ax.tick_params(axis='both', which='major', labelsize=64)\n",
    "            fig.savefig(SAVED_PLOTS_LOCATION_LOOP + '/' + str(df_sub_loop.iloc[ind, 0][0]) + '_' + str(df_sub_loop.iloc[ind, 0][1]) + '.png',bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            # Save corresponding vega diagram along with feature plot\n",
    "            srvc = dict(service=df_sub_loop.iloc[ind, 0][0],direction=df_sub_loop.iloc[ind, 0][1])\n",
    "            service_route = route.for_service(**srvc)\n",
    "            trips = (ezlink.for_service(**srvc).get_trips(service_route))\n",
    "            edges = trips.to_edges(source=\"src_seq\", target=\"dst_seq\", value=\"pax\")\n",
    "            nodes = service_route.to_nodes(name=\"stop_code\", index=\"seq\", order=\"km\")\n",
    "            arc_html = vega(arc_diagram(edges=edges, nodes=nodes), render=False)\n",
    "            arc_file = open(SAVED_PLOTS_LOCATION_LOOP + '/' + str(df_sub_loop.iloc[ind, 0][0]) + '_' + str(df_sub_loop.iloc[ind, 0][1]) + '_vega_arc.html', \"w\")\n",
    "            arc_file.write(arc_html)\n",
    "            arc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes can be further optimized. For instance, the plotting functions above can be generalized into functions and using a package like _concurrent.futures_, parallel processing can be used to speed up generation and saving of the plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results generated from this approach did generate clusters that were meaningful but unfortunately, it also showed that \"global\" interventions might not be best suited to improve commuter experience across these services."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
